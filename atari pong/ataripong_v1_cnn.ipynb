{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://towardsdatascience.com/getting-an-ai-to-play-atari-pong-with-deep-reinforcement-learning-47b0c56e78ae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STRATEGY:\n",
    "1. Take a tensor of pixel values from the 4 most recent frames to be the __current state__.\n",
    "2. Using the epsilon-greedy strategy, take a random action or input __current state__ into the __CNN__ to get the next action.\n",
    "3. Perform the action, receive a reward, and arrive at the next state. Store values in memory for training.\n",
    "4. After each action, randomly sample data from the agent's memory and train the agent against a __loss__ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESS FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def resize_frame(frame):\n",
    "    # STEP 1: Crop the image, and convert to grayscale.\n",
    "    frame = frame[30:-12, 5:-4]\n",
    "    frame = np.average(frame, axis=2)\n",
    "    # STEP 2: Resize the frame using \"nearest-neighbour interpolation\".\n",
    "    # This method of interpolation takes the rounded value of the expected position, and finds the closest data value at integer position.\n",
    "    frame = cv2.resize(frame, (84, 84), interpolation=cv2.INTER_NEAREST)\n",
    "    # STEP 3: Convert image datatype to np.uint8 (unsigned integer).\n",
    "    frame = np.array(frame, dtype=np.uint8)\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# \"deque\" stands for double-ended queue. it's essentially a stack of things.\n",
    "# In this Memory class, we have 4 separate deques that contain frames, actions, rewards, and done-flags.\n",
    "class Memory():\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "        self.frames = deque(maxlen=max_len)\n",
    "        self.actions = deque(maxlen=max_len)\n",
    "        self.rewards = deque(maxlen=max_len)\n",
    "        self.done_flags = deque(maxlen=max_len)\n",
    "        \n",
    "    # This add_experience function simply adds new experiences to each of the 4 deques created above.\n",
    "    def add_experience(self, next_frame, next_frames_reward, next_action, next_frame_terminal):\n",
    "        self.frames.append(next_frame)\n",
    "        self.actions.append(next_action)\n",
    "        self.rewards.append(next_frames_reward)\n",
    "        self.done_flags.append(next_frame_terminal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "#import preprocess_frame as ppf\n",
    "import numpy as np\n",
    "\n",
    "# This function resets the environment, gets the starting frame, and declares a dummy action + reward.\n",
    "def initialize_new_game(name, env, agent):\n",
    "    \"\"\"We don't want an agents past game influencing its new game, so we add in some dummy data to initialize\"\"\"\n",
    "\n",
    "    env.reset()\n",
    "    starting_frame = resize_frame(env.step(0)[0])  # refer to OG doc\n",
    "\n",
    "    dummy_action = 0\n",
    "    dummy_reward = 0\n",
    "    dummy_done = False\n",
    "    # Recall that we're using 4 frames stacked together for the agent to understand what's happening, so we need to repeat this action 3 more times.\n",
    "    for i in range(3):\n",
    "        agent.memory.add_experience(\n",
    "            starting_frame, dummy_reward, dummy_action, dummy_done)\n",
    "\n",
    "# This function gets the environment from Open AI Gym.\n",
    "def make_env(name, agent):\n",
    "    env = gym.make(name, render_mode='human')  # added render_mode\n",
    "    return env\n",
    "\n",
    "# This function is where the actual playing of the game occurs.\n",
    "# The agent performs some action, the weights and scores are documented and updated, we get new frames for the next action, and we add the experience to the memory until the game is over.\n",
    "def take_step(name, env, agent, score, debug):\n",
    "\n",
    "    # 1 and 2: Update timesteps and save weights\n",
    "    agent.total_timesteps += 1\n",
    "    if agent.total_timesteps % 50000 == 0:\n",
    "        agent.model.save_weights('recent_weights.hdf5')\n",
    "        print('\\nWeights saved!')\n",
    "\n",
    "    # 3: Take action\n",
    "    # env.step() takes a step in the environment by performing an action.\n",
    "    # In return, we get the next frame, reward, a done flag, and info. The done flag represents the progress of the game (finished or ongoing).\n",
    "    next_frame, next_frames_reward, next_frame_terminal, info = env.step(\n",
    "        agent.memory.actions[-1])\n",
    "\n",
    "    # 4: Get next state\n",
    "    next_frame = resize_frame(next_frame)  # refer to OG doc\n",
    "    new_state = [agent.memory.frames[-3], agent.memory.frames[-2],\n",
    "                 agent.memory.frames[-1], next_frame]\n",
    "    # We have to do this to get it into keras's goofy format of [batch_size,rows,columns,channels]\n",
    "    new_state = np.moveaxis(new_state, 0, 2)/255\n",
    "    new_state = np.expand_dims(new_state, 0)  # ^^^\n",
    "\n",
    "    # 5: Get next action, using next state\n",
    "    next_action = agent.get_action(new_state)\n",
    "\n",
    "    # 6: If game is over, return the score\n",
    "    if next_frame_terminal:\n",
    "        agent.memory.add_experience(\n",
    "            next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
    "        return (score + next_frames_reward), True\n",
    "\n",
    "    # 7: Now we add the next experience to memory\n",
    "    agent.memory.add_experience(\n",
    "        next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
    "\n",
    "    # 8: If we are trying to debug this then render\n",
    "    if debug:\n",
    "        env.render()\n",
    "\n",
    "    # 9: If the threshold memory is satisfied, make the agent learn from memory\n",
    "    if len(agent.memory.frames) > agent.starting_mem_len:\n",
    "        agent.learn(debug)\n",
    "\n",
    "    return (score + next_frames_reward), False\n",
    "\n",
    "# This function calls the take_step() function until an episode is completed.\n",
    "def play_episode(name, env, agent, debug=False):\n",
    "    initialize_new_game(name, env, agent)\n",
    "    done = False\n",
    "    score = 0\n",
    "    while True:\n",
    "        score, done = take_step(name, env, agent, score, debug)\n",
    "        if done:\n",
    "            break\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGENT CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this class, we describe the actions of the agent and create the CNN.\n",
    "- For this CNN, we are using the __Huber loss__ function which tends to be less sensitive to outliers in data than the mean squared error loss function.\n",
    "- We are also using __Adam__ for our optimiser.\n",
    "    - __Stochastic gradient descent (SGD)__ is the usual optimiser that we've used previously.\n",
    "        - In SGD, we have a single __learning rate (alpha)__ for all weight updates, and the learning rate does NOT change during training.\n",
    "    - But Adam (_adaptive moment estimation_) \"adapts\" the learning rate, which means that training for the agent can become more precise and time-saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential, clone_model\n",
    "# changed from Input to InputLayer\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, InputLayer\n",
    "from tensorflow.python.keras.optimizer_v1 import Adam  # refer to OG doc\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "#from agent_memory import Memory\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, possible_actions, starting_mem_len, max_mem_len, starting_epsilon, learn_rate, starting_lives=5, debug=False):\n",
    "        self.memory = Memory(max_mem_len)\n",
    "        self.possible_actions = possible_actions\n",
    "        self.epsilon = starting_epsilon\n",
    "        self.epsilon_decay = .9/100000\n",
    "        self.epsilon_min = .05\n",
    "        self.gamma = .95\n",
    "        self.learn_rate = learn_rate\n",
    "        self.model = self._build_model()\n",
    "        self.model_target = clone_model(self.model)\n",
    "        self.total_timesteps = 0\n",
    "        self.lives = starting_lives  # this parameter does not apply to pong\n",
    "        self.starting_mem_len = starting_mem_len\n",
    "        self.learns = 0\n",
    "\n",
    "    # This is where the CNN gets constructed.\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer((84, 84, 4)))  # changed from Input to InputLayer\n",
    "        model.add(Conv2D(filters=32, kernel_size=(8, 8), strides=4, data_format=\"channels_last\",\n",
    "                  activation='relu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Conv2D(filters=64, kernel_size=(4, 4), strides=2, data_format=\"channels_last\",\n",
    "                  activation='relu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=1, data_format=\"channels_last\",\n",
    "                  activation='relu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu',\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Dense(len(self.possible_actions), activation='linear'))\n",
    "        optimizer = Adam(self.learn_rate)\n",
    "        model.compile(optimizer, loss=tf.keras.losses.Huber())\n",
    "        model.summary()\n",
    "        print('\\nAgent Initialized\\n')\n",
    "        return model\n",
    "\n",
    "    # This is where we implement the epsilon-greedy strategy.\n",
    "    # We generate a random number, and if it is less than our epsilon value, we take a random action.\n",
    "    # If not, we pass the current state into our CNN and return the max output.\n",
    "    def get_action(self, state):\n",
    "        \"\"\"Explore\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.sample(self.possible_actions, 1)[0]\n",
    "\n",
    "        \"\"\"Do Best Acton\"\"\"\n",
    "        a_index = np.argmax(self.model.predict(state))\n",
    "        return self.possible_actions[a_index]\n",
    "\n",
    "    # This function makes sure that the frame collected is not from 2 different games\n",
    "    def _index_valid(self, index):\n",
    "        if self.memory.done_flags[index-3] or self.memory.done_flags[index-2] or self.memory.done_flags[index-1] or self.memory.done_flags[index]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    # Notice on Line 24 that we've also created a clone of the CNN and called it model_target.\n",
    "    # This is a method used to decrease training noise.\n",
    "    ## During training, we input the next state into our CNN to help generate the target for the error function. After the weights are updated, the target within the error function will change since the CNN is being used to generate the target, and even though the CNN weights were just changed.\n",
    "    ## So, instead, we create a clone of the CNN and use this model_target to generate the targets for our loss function, and every so often, we update the model_target to match our original CNN.\n",
    "    ## This method decreases training time!\n",
    "    def learn(self, debug=False):\n",
    "        \"\"\"we want the output[a] to be R_(t+1) + Qmax_(t+1).\"\"\"\n",
    "        \"\"\"So target for taking action 1 should be [output[0], R_(t+1) + Qmax_(t+1), output[2]]\"\"\"\n",
    "\n",
    "        \"\"\"First we need 32 random valid indicies\"\"\"\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions_taken = []\n",
    "        next_rewards = []\n",
    "        next_done_flags = []\n",
    "\n",
    "        while len(states) < 32:\n",
    "            index = np.random.randint(4, len(self.memory.frames) - 1)\n",
    "            if self._index_valid(index):\n",
    "                state = [self.memory.frames[index-3], self.memory.frames[index-2],\n",
    "                         self.memory.frames[index-1], self.memory.frames[index]]\n",
    "                state = np.moveaxis(state, 0, 2)/255\n",
    "                next_state = [self.memory.frames[index-2], self.memory.frames[index-1],\n",
    "                              self.memory.frames[index], self.memory.frames[index+1]]\n",
    "                next_state = np.moveaxis(next_state, 0, 2)/255\n",
    "\n",
    "                states.append(state)\n",
    "                next_states.append(next_state)\n",
    "                actions_taken.append(self.memory.actions[index])\n",
    "                next_rewards.append(self.memory.rewards[index+1])\n",
    "                next_done_flags.append(self.memory.done_flags[index+1])\n",
    "\n",
    "        \"\"\"Now we get the ouputs from our model, and the target model. We need this for our target in the error function\"\"\"\n",
    "        labels = self.model.predict(np.array(states))\n",
    "        next_state_values = self.model_target.predict(np.array(next_states))\n",
    "\n",
    "        \"\"\"Now we define our labels, or what the output should have been\n",
    "           We want the output[action_taken] to be R_(t+1) + Qmax_(t+1) \"\"\"\n",
    "        for i in range(32):\n",
    "            action = self.possible_actions.index(actions_taken[i])\n",
    "            labels[i][action] = next_rewards[i] + \\\n",
    "                (not next_done_flags[i]) * \\\n",
    "                self.gamma * max(next_state_values[i])\n",
    "\n",
    "        \"\"\"Train our model using the states and outputs generated\"\"\"\n",
    "        self.model.fit(np.array(states), labels,\n",
    "                       batch_size=32, epochs=1, verbose=0)\n",
    "\n",
    "        \"\"\"Decrease epsilon and update how many times our agent has learned\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "        self.learns += 1\n",
    "\n",
    "        \"\"\"Every 10000 learned, copy our model weights to our target model\"\"\"\n",
    "        if self.learns % 10000 == 0:\n",
    "            self.model_target.set_weights(self.model.get_weights())\n",
    "            print('\\nTarget model updated')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 1,685,667\n",
      "Trainable params: 1,685,667\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Agent Initialized\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vanya\\OneDrive\\Desktop\\ANT61\\atari pong\\ataripong_v1_cnn.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000012?line=25'>26</a>\u001b[0m timee \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000012?line=26'>27</a>\u001b[0m \u001b[39m# set debug to true for rendering\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000012?line=27'>28</a>\u001b[0m score \u001b[39m=\u001b[39m play_episode(name, env, agent, debug\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000012?line=28'>29</a>\u001b[0m scores\u001b[39m.\u001b[39mappend(score)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000012?line=29'>30</a>\u001b[0m \u001b[39mif\u001b[39;00m score \u001b[39m>\u001b[39m max_score:\n",
      "\u001b[1;32mc:\\Users\\vanya\\OneDrive\\Desktop\\ANT61\\atari pong\\ataripong_v1_cnn.ipynb Cell 8'\u001b[0m in \u001b[0;36mplay_episode\u001b[1;34m(name, env, agent, debug)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000007?line=75'>76</a>\u001b[0m score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000007?line=76'>77</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000007?line=77'>78</a>\u001b[0m     score, done \u001b[39m=\u001b[39m take_step(name, env, agent, score, debug)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000007?line=78'>79</a>\u001b[0m     \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000007?line=79'>80</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\vanya\\OneDrive\\Desktop\\ANT61\\atari pong\\ataripong_v1_cnn.ipynb Cell 8'\u001b[0m in \u001b[0;36mtake_step\u001b[1;34m(name, env, agent, score, debug)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000007?line=32'>33</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mWeights saved!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000007?line=34'>35</a>\u001b[0m \u001b[39m# 3: Take action\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000007?line=35'>36</a>\u001b[0m \u001b[39m# env.step() takes a step in the environment by performing an action.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000007?line=36'>37</a>\u001b[0m \u001b[39m# In return, we get the next frame, reward, a done flag, and info. The done flag represents the progress of the game (finished or ongoing).\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000007?line=37'>38</a>\u001b[0m next_frame, next_frames_reward, next_frame_terminal, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000007?line=38'>39</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49mactions[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000007?line=40'>41</a>\u001b[0m \u001b[39m# 4: Get next state\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanya/OneDrive/Desktop/ANT61/atari%20pong/ataripong_v1_cnn.ipynb#ch0000007?line=41'>42</a>\u001b[0m next_frame \u001b[39m=\u001b[39m resize_frame(next_frame)  \u001b[39m# refer to OG doc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vanya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/vanya/AppData/Local/Programs/Python/Python310/lib/site-packages/gym/wrappers/time_limit.py?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     <a href='file:///c%3A/Users/vanya/AppData/Local/Programs/Python/Python310/lib/site-packages/gym/wrappers/time_limit.py?line=14'>15</a>\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m     <a href='file:///c%3A/Users/vanya/AppData/Local/Programs/Python/Python310/lib/site-packages/gym/wrappers/time_limit.py?line=15'>16</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/vanya/AppData/Local/Programs/Python/Python310/lib/site-packages/gym/wrappers/time_limit.py?line=16'>17</a>\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='file:///c%3A/Users/vanya/AppData/Local/Programs/Python/Python310/lib/site-packages/gym/wrappers/time_limit.py?line=17'>18</a>\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='file:///c%3A/Users/vanya/AppData/Local/Programs/Python/Python310/lib/site-packages/gym/wrappers/time_limit.py?line=18'>19</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/vanya/AppData/Local/Programs/Python/Python310/lib/site-packages/gym/wrappers/time_limit.py?line=19'>20</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\vanya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\atari\\environment.py:238\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[1;34m(self, action_ind)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/vanya/AppData/Local/Programs/Python/Python310/lib/site-packages/gym/envs/atari/environment.py?line=235'>236</a>\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/vanya/AppData/Local/Programs/Python/Python310/lib/site-packages/gym/envs/atari/environment.py?line=236'>237</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(frameskip):\n\u001b[1;32m--> <a href='file:///c%3A/Users/vanya/AppData/Local/Programs/Python/Python310/lib/site-packages/gym/envs/atari/environment.py?line=237'>238</a>\u001b[0m     reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39male\u001b[39m.\u001b[39mact(action)\n\u001b[0;32m    <a href='file:///c%3A/Users/vanya/AppData/Local/Programs/Python/Python310/lib/site-packages/gym/envs/atari/environment.py?line=239'>240</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_obs(), reward, terminal, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_info()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "#name = 'PongDeterministic-v4'\n",
    "name = 'Pong-v4'\n",
    "agent = Agent(possible_actions=[0, 2, 3], starting_mem_len=50000,\n",
    "              max_mem_len=750000, starting_epsilon=1, learn_rate=.00025)\n",
    "env = make_env(name, agent)\n",
    "\n",
    "last_100_avg = [-21]\n",
    "scores = deque(maxlen=100)\n",
    "max_score = -21\n",
    "\n",
    "\"\"\" If testing:\n",
    "agent.model.load_weights('recent_weights.hdf5')\n",
    "agent.model_target.load_weights('recent_weights.hdf5')\n",
    "agent.epsilon = 0.0\n",
    "\"\"\"\n",
    "\n",
    "env.reset()\n",
    "\n",
    "for i in range(10000):  # changed from 1000000 to 10000\n",
    "    timesteps = agent.total_timesteps\n",
    "    timee = time.time()\n",
    "    # set debug to true for rendering\n",
    "    score = play_episode(name, env, agent, debug=False)\n",
    "    scores.append(score)\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "\n",
    "    print('\\nEpisode: ' + str(i))\n",
    "    print('Steps: ' + str(agent.total_timesteps - timesteps))\n",
    "    print('Duration: ' + str(time.time() - timee))\n",
    "    print('Score: ' + str(score))\n",
    "    print('Max Score: ' + str(max_score))\n",
    "    print('Epsilon: ' + str(agent.epsilon))\n",
    "\n",
    "    if i % 100 == 0 and i != 0:\n",
    "        last_100_avg.append(sum(scores)/len(scores))\n",
    "        plt.plot(np.arange(0, i+1, 100), last_100_avg)\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8732c892ed25220206e2171fcd9341bbbe55a647096787aa5baa73044076264"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
